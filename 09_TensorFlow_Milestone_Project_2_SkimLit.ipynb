{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmaresza/tensorflow_course/blob/main/09_TensorFlow_Milestone_Project_2_SkimLit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9TGZMRFL4ON"
      },
      "source": [
        "# Milestone Project 2: SkimLit\n",
        "\n",
        "The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n",
        "\n",
        "The paper we'll be replicating (the source of the dataset that we'll be using) is available here: https://arxiv.org/pdf/1710.06071\n",
        "\n",
        "Reading through the paper above, we see that the model architecture that was used to achieve the best results is available here: https://arxiv.org/pdf/1612.05251\n",
        "\n",
        "Much of the work was done in the [lesson notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/video_notebooks/09_SkimLit_nlp_milestone_project_2_video.ipynb), and is continued in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY5YJjeocgsJ"
      },
      "source": [
        "### Get data and preprocess it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VLgq-IRR84W2",
        "outputId": "8a9838eb-ca17-4cb8-dcd5-bc4b0b54f2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "# Had to install old tensorflow version to make sure code ran properly\n",
        "!pip install tensorflow==2.15.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing tensorflow_text for Exercise #5\n",
        "!pip install tensorflow_text==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBIgegx0zQS_",
        "outputId": "096ddec2-a8c5-49c2-a6ca-a91706276cbb",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text==2.15.0\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text==2.15.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow_text==2.15.0) (2.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.13.0->tensorflow_text==2.15.0)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow_text==2.15.0) (3.2.2)\n",
            "Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-keras, tensorflow_text\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "Successfully installed tensorflow_text-2.15.0 tf-keras-2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WcqEqxy8ZKuL"
      },
      "outputs": [],
      "source": [
        "# Useful imports\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPx6qqgObPmE",
        "outputId": "f6cfa948-1a77-4690-ce98-f40201e0cb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 15:02:01--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-08-14 15:02:01 (94.9 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get helper functions\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "from helper_functions import calculate_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3MJWq6Acdvw",
        "outputId": "21782b6a-03aa-4dc6-f8a4-5ff420d4bde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pubmed-rct'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 39 (delta 8), reused 5 (delta 5), pack-reused 25 (from 1)\u001b[K\n",
            "Receiving objects: 100% (39/39), 177.08 MiB | 18.17 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "Updating files: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "# Download PubMed 200k RCT data from GitHub\n",
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "\n",
        "# Use the 20k dataset with numbers replaced by '@' sign for the experiments\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vFl5x4fadm_M"
      },
      "outputs": [],
      "source": [
        "# Function to read and format data before further preprocessing\n",
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"\n",
        "  Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents, and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract, and what sentence\n",
        "  number the target line is.\n",
        "  \"\"\"\n",
        "  with open(filename, \"r\") as f:\n",
        "    input_lines = f.readlines()\n",
        "  # input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "\n",
        "  # Loop through each line in the target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if the line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
        "\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in a single abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create an empty dictionary for each line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and make it lowercase\n",
        "        line_data[\"line_number\"] = abstract_line_number # in which position does the line appear within the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are there in the target abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    else: # if the above condiditons aren't fulfilled, the line contains a labeled sentence\n",
        "      abstract_lines += line\n",
        "\n",
        "  return abstract_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1itTbU7igZec",
        "outputId": "15dfe074-a006-4956-c44d-ffec78789be1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'target': 'OBJECTIVE',\n",
              "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
              "  'line_number': 0,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
              "  'line_number': 1,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
              "  'line_number': 2,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
              "  'line_number': 3,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
              "  'line_number': 4,\n",
              "  'total_lines': 11}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Get data from files and preprocess it\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IREnNKI3ggNo",
        "outputId": "0eaa733a-7dca-476c-a5e0-c73b5903ce8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      target                                               text  line_number  \\\n",
              "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
              "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
              "2    METHODS  outcome measures included pain reduction and i...            2   \n",
              "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
              "4    METHODS  secondary outcome measures included the wester...            4   \n",
              "\n",
              "   total_lines  \n",
              "0           11  \n",
              "1           11  \n",
              "2           11  \n",
              "3           11  \n",
              "4           11  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0cb91da-3869-4fa9-abcb-daacc8362024\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0cb91da-3869-4fa9-abcb-daacc8362024')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0cb91da-3869-4fa9-abcb-daacc8362024 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0cb91da-3869-4fa9-abcb-daacc8362024');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63902598-9da6-47d0-8a7c-9860e641603c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63902598-9da6-47d0-8a7c-9860e641603c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63902598-9da6-47d0-8a7c-9860e641603c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Turn data into DataFrames\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp2ifNx8goMG",
        "outputId": "57ee8fcf-8e68-476e-cd0f-846e5208ac8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Convert abstract text lines into lists\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHdGfwYygwQz",
        "outputId": "a57b7106-f469-4a1d-c06f-e34424ef3305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode labels (for training)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse=False) # we want a non-sparse matrix\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ydz8Ertkgx1r"
      },
      "outputs": [],
      "source": [
        "# Label encode labels (for prediction metrics)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q1uRTLOH-hN",
        "outputId": "5adf6773-9d0e-4892-9aea-7a225c9af1d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Get class names and number of classes\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzqNnvCYMS2o"
      },
      "source": [
        "## 1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving.\n",
        "Since this might take a while, you might want to use:\n",
        "* `tf.keras.callbacks.ModelCheckpoint` to save the model's best weights only\n",
        "* `tf.keras.callbacks.EarlyStopping` to stop the model from training once the validation loss has stopped improving for ~3 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25-oI8mHprJn"
      },
      "source": [
        "### Turn train and validation data into tf.data Datasets for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "4Z7O9_HipPUC"
      },
      "outputs": [],
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [\" \".join(list(sentence)) for sentence in train_sentences]\n",
        "val_chars = [\" \".join(list(sentence)) for sentence in val_sentences]\n",
        "\n",
        "# Create one-hot-encoded tensors of the \"line_number\" column\n",
        "train_line_numbers = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "\n",
        "# Create one-hot-encoded tensors of the \"total_lines\" feature\n",
        "train_total_lines = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "\n",
        "# Create train dataset (with all four kinds of input data)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_line_numbers,\n",
        "                                                 train_total_lines,\n",
        "                                                 train_sentences,\n",
        "                                                 train_chars))\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_dataset = tf.data.Dataset.zip((train_data, train_labels))\n",
        "train_dataset = train_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Do the same as above but for the validation dataset\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val_line_numbers,\n",
        "                                               val_total_lines,\n",
        "                                               val_sentences,\n",
        "                                               val_chars))\n",
        "val_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_dataset = tf.data.Dataset.zip((val_data, val_labels))\n",
        "val_dataset = val_dataset.batch(512).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hYD5yUtOPQQ",
        "outputId": "8e394042-14ec-4b10-9bd2-9bd7e9407bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 15:02:22--  https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.79.207, 108.177.96.207, 108.177.119.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.79.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 962182847 (918M) [application/zip]\n",
            "Saving to: ‘skimlit_tribrid_model.zip’\n",
            "\n",
            "skimlit_tribrid_mod 100%[===================>] 917.61M  25.0MB/s    in 36s     \n",
            "\n",
            "2024-08-14 15:02:58 (25.6 MB/s) - ‘skimlit_tribrid_model.zip’ saved [962182847/962182847]\n",
            "\n",
            "Archive:  skimlit_tribrid_model.zip\n",
            "   creating: skimlit_gs_model/skimlit_tribrid_model/\n",
            "  inflating: skimlit_gs_model/skimlit_tribrid_model/keras_metadata.pb  \n",
            "   creating: skimlit_gs_model/skimlit_tribrid_model/assets/\n",
            " extracting: skimlit_gs_model/skimlit_tribrid_model/fingerprint.pb  \n",
            "   creating: skimlit_gs_model/skimlit_tribrid_model/variables/\n",
            "  inflating: skimlit_gs_model/skimlit_tribrid_model/variables/variables.index  \n",
            "  inflating: skimlit_gs_model/skimlit_tribrid_model/variables/variables.data-00000-of-00001  \n",
            "  inflating: skimlit_gs_model/skimlit_tribrid_model/saved_model.pb  \n"
          ]
        }
      ],
      "source": [
        "# Download saved version of model_5\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\n",
        "!mkdir skimlit_gs_model\n",
        "!unzip skimlit_tribrid_model.zip -d skimlit_gs_model\n",
        "!rm skimlit_tribrid_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akfk-w41OVDY",
        "outputId": "a374b308-dda9-49e1-a085-804cc6a1be2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " char_inputs (InputLayer)    [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " token_inputs (InputLayer)   [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " char_vectorizer (TextVecto  (None, 290)                  0         ['char_inputs[0][0]']         \n",
            " rization)                                                                                        \n",
            "                                                                                                  \n",
            " universal_sentence_encoder  (None, 512)                  2567978   ['token_inputs[0][0]']        \n",
            "  (KerasLayer)                                            24                                      \n",
            "                                                                                                  \n",
            " char_embed (Embedding)      (None, 290, 25)              1750      ['char_vectorizer[0][0]']     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 128)                  65664     ['universal_sentence_encoder[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  (None, 64)                   14848     ['char_embed[0][0]']          \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " token_char_hybrid_embeddin  (None, 192)                  0         ['dense_7[0][0]',             \n",
            " g (Concatenate)                                                     'bidirectional_1[0][0]']     \n",
            "                                                                                                  \n",
            " line_number_input (InputLa  [(None, 15)]                 0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " total_lines_input (InputLa  [(None, 20)]                 0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 256)                  49408     ['token_char_hybrid_embedding[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 32)                   512       ['line_number_input[0][0]']   \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 32)                   672       ['total_lines_input[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 256)                  0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " token_char_positional_embe  (None, 320)                  0         ['dense_8[0][0]',             \n",
            " dding (Concatenate)                                                 'dense_9[0][0]',             \n",
            "                                                                     'dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " output_layer (Dense)        (None, 5)                    1605      ['token_char_positional_embedd\n",
            "                                                                    ing[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 256932283 (980.12 MB)\n",
            "Trainable params: 134459 (525.23 KB)\n",
            "Non-trainable params: 256797824 (979.61 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Load in downloaded model\n",
        "model_5 = tf.keras.models.load_model(\"skimlit_gs_model/skimlit_tribrid_model/\")\n",
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bnxY4r4rpFHL"
      },
      "outputs": [],
      "source": [
        "# Create ModelCheckpoint callback\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"model_5\",\n",
        "                                                      monitor=\"val_loss\",\n",
        "                                                      save_weights_only=True,\n",
        "                                                      save_best_only=True)\n",
        "\n",
        "# Create EarlyStopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                              patience=3,\n",
        "                                              restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsxU9DdfqnzX",
        "outputId": "fa413106-43d5-46b9-fa69-b803391e7e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "352/352 [==============================] - 64s 142ms/step - loss: 0.4351 - accuracy: 0.8390 - val_loss: 0.3930 - val_accuracy: 0.8497\n",
            "Epoch 2/100\n",
            "352/352 [==============================] - 42s 118ms/step - loss: 0.3858 - accuracy: 0.8547 - val_loss: 0.3701 - val_accuracy: 0.8574\n",
            "Epoch 3/100\n",
            "352/352 [==============================] - 50s 141ms/step - loss: 0.3628 - accuracy: 0.8638 - val_loss: 0.3573 - val_accuracy: 0.8611\n",
            "Epoch 4/100\n",
            "352/352 [==============================] - 49s 138ms/step - loss: 0.3459 - accuracy: 0.8703 - val_loss: 0.3507 - val_accuracy: 0.8645\n",
            "Epoch 5/100\n",
            "352/352 [==============================] - 52s 147ms/step - loss: 0.3325 - accuracy: 0.8754 - val_loss: 0.3461 - val_accuracy: 0.8662\n",
            "Epoch 6/100\n",
            "352/352 [==============================] - 49s 140ms/step - loss: 0.3215 - accuracy: 0.8800 - val_loss: 0.3445 - val_accuracy: 0.8674\n",
            "Epoch 7/100\n",
            "352/352 [==============================] - 50s 141ms/step - loss: 0.3120 - accuracy: 0.8842 - val_loss: 0.3435 - val_accuracy: 0.8688\n",
            "Epoch 8/100\n",
            "352/352 [==============================] - 42s 120ms/step - loss: 0.3036 - accuracy: 0.8871 - val_loss: 0.3425 - val_accuracy: 0.8692\n",
            "Epoch 9/100\n",
            "352/352 [==============================] - 35s 99ms/step - loss: 0.2953 - accuracy: 0.8896 - val_loss: 0.3437 - val_accuracy: 0.8691\n",
            "Epoch 10/100\n",
            "352/352 [==============================] - 36s 103ms/step - loss: 0.2880 - accuracy: 0.8926 - val_loss: 0.3447 - val_accuracy: 0.8692\n",
            "Epoch 11/100\n",
            "352/352 [==============================] - 35s 98ms/step - loss: 0.2821 - accuracy: 0.8949 - val_loss: 0.3484 - val_accuracy: 0.8679\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model_5.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "model_5_history = model_5.fit(train_dataset,\n",
        "                              epochs=100,\n",
        "                              validation_data=val_dataset,\n",
        "                              callbacks=[model_checkpoint,\n",
        "                                         early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hOsLl75LXew",
        "outputId": "000687ec-1ac2-41a1-c756-fc50f4965447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 6s 73ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 86.92241493446313,\n",
              " 'precision': 0.8733841500780213,\n",
              " 'recall': 0.8692241493446313,\n",
              " 'f1': 0.8663665483557124}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Make predictions with the model\n",
        "model_5_pred_probs = model_5.predict(val_dataset, verbose=1)\n",
        "\n",
        "# Convert pred probs to pred labels\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "\n",
        "# Calculate results of model_5\n",
        "model_5_val_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                        y_pred=model_5_preds)\n",
        "model_5_val_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trained on all of the data, the model was able to achieve nearly 87% accuracy on the validation dataset, compared to ~83% accuracy when trained on only 10% of the data."
      ],
      "metadata": {
        "id": "Eq9pOqcDZ9Ri"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0e9hh38ZEr9"
      },
      "source": [
        "## 2. Turn the test data samples into a tf.data Dataset (fast loading) and then evaluate the best performing model (`model_5`) on the test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Fccb2YvFhx_R"
      },
      "outputs": [],
      "source": [
        "# Split sequence-level data split into character-level data split\n",
        "test_chars = [\" \".join(list(sentence)) for sentence in test_sentences]\n",
        "\n",
        "# Create one-hot-encoded tensors of the \"line_number\" column\n",
        "test_line_numbers = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "\n",
        "# Create one-hot-encoded tensors of the \"total_lines\" feature\n",
        "test_total_lines = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "\n",
        "# Create test dataset (with all four kinds of input data)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_line_numbers,\n",
        "                                                test_total_lines,\n",
        "                                                test_sentences,\n",
        "                                                test_chars))\n",
        "test_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_dataset = tf.data.Dataset.zip((test_data, test_labels))\n",
        "test_dataset = test_dataset.batch(512).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFVq90LHnFl3",
        "outputId": "d6f98d44-a963-4586-8afa-5beceaaecb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 4s 63ms/step - loss: 0.3621 - accuracy: 0.8630\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3620966970920563, 0.862983226776123]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Evaluate model_5 on the test data\n",
        "model_5.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hy3Ys69Fu8K2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa06375-67a2-4842-b30d-8b0db963525f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 5s 82ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 86.29832420773187,\n",
              " 'precision': 0.8658495260268554,\n",
              " 'recall': 0.8629832420773187,\n",
              " 'f1': 0.8601149553742251}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Make predictions with the model\n",
        "model_5_pred_probs = model_5.predict(test_dataset, verbose=1)\n",
        "\n",
        "# Convert pred probs to pred labels\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "\n",
        "# Calculate results of model_5\n",
        "model_5_test_results = calculate_results(y_true=test_labels_encoded,\n",
        "                                         y_pred=model_5_preds)\n",
        "model_5_test_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting on the test dataset gives very similar results to predicting on the validation dataset, although for some reason it seems that the validation dataset yields slightly better results."
      ],
      "metadata": {
        "id": "dTYeE49caPFM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P54V1OvPZLZR"
      },
      "source": [
        "## 3. Find the most wrong predictions from exercise 2 (these are the samples where the model has predicted the wrong label with the highest prediction probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "khgI0Z0QZaTp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f3af17e2-f080-42c3-dd46-0be11286e147",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 4s 64ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text      target       pred  \\\n",
              "0  this study analyzed liver function abnormaliti...  BACKGROUND  OBJECTIVE   \n",
              "1  a post hoc analysis was conducted with the use...     RESULTS    METHODS   \n",
              "2  liver function tests ( lfts ) were measured at...     RESULTS    METHODS   \n",
              "3  survival analyses were used to assess the asso...     RESULTS    METHODS   \n",
              "4  the percentage of patients with abnormal lfts ...     RESULTS    RESULTS   \n",
              "\n",
              "   pred_prob  \n",
              "0   0.511526  \n",
              "1   0.698331  \n",
              "2   0.988842  \n",
              "3   0.966783  \n",
              "4   0.976663  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8acb485-a67a-41eb-8d14-a28fd2255921\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this study analyzed liver function abnormaliti...</td>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>0.511526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a post hoc analysis was conducted with the use...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.698331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>liver function tests ( lfts ) were measured at...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.988842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>survival analyses were used to assess the asso...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.966783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the percentage of patients with abnormal lfts ...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.976663</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8acb485-a67a-41eb-8d14-a28fd2255921')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8acb485-a67a-41eb-8d14-a28fd2255921 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8acb485-a67a-41eb-8d14-a28fd2255921');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5be22aac-2602-41de-b4bd-64bdad3f275f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5be22aac-2602-41de-b4bd-64bdad3f275f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5be22aac-2602-41de-b4bd-64bdad3f275f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_preds_df",
              "summary": "{\n  \"name\": \"test_preds_df\",\n  \"rows\": 30135,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29735,\n        \"samples\": [\n          \"in this secondary study of an open-label , @-week , randomized , controlled trial conducted between april @ and august @ , clinically stable patients with schizophrenia ( dsm-iv ) treated with risperidone or olanzapine were randomly assigned to the reduction group ( dose reduced by @ % ) or maintenance group ( dose kept constant ) .\",\n          \"between july , @ , and june , @ , we did a multinational double-blind , randomised , phase @ study comparing vacc-@x with placebo .\",\n          \"a sample of @ participants was interviewed over the telephone for possible cognitive and background moderators .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RESULTS\",\n          \"OBJECTIVE\",\n          \"CONCLUSIONS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"METHODS\",\n          \"BACKGROUND\",\n          \"RESULTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_prob\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 29279,\n        \"samples\": [\n          0.9998214840888977,\n          0.9795616269111633,\n          0.676633358001709\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "model_5_pred_probs = model_5.predict(test_dataset)\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "pred_probs = model_5_pred_probs.max(axis=1)\n",
        "model_5_labels = [class_names[i] for i in model_5_preds]\n",
        "\n",
        "# Create DataFrame with test sentences, target labels, predicted labels, and prediction probabilities\n",
        "test_preds_df = pd.DataFrame({\"text\": test_df[\"text\"],\n",
        "                              \"target\": test_df[\"target\"],\n",
        "                              \"pred\": model_5_labels,\n",
        "                              \"pred_prob\": pred_probs})\n",
        "test_preds_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Find out which predictions are wrong in our DataFrame\n",
        "test_preds_df[\"pred_correct\"] = test_preds_df[\"target\"] == test_preds_df[\"pred\"]"
      ],
      "metadata": {
        "id": "s6b7k9d9QWMq"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new DataFrame of 100 most wrong predictions\n",
        "top_100_wrong = test_preds_df[test_preds_df[\"pred_correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100]\n",
        "top_100_wrong.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "LvlcXeS0QNnB",
        "outputId": "9afa74ce-d5eb-4dd0-8f02-8c77ffa9f623"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text       target  \\\n",
              "13598  -@ % vs. fish : -@ % vs. fish + s : -@ % ; p <...      METHODS   \n",
              "1827                       nct@ ( clinicaltrials.gov ) .  CONCLUSIONS   \n",
              "25664  rifampicin significantly increased the mean ar...  CONCLUSIONS   \n",
              "3573   a cluster randomised trial was implemented wit...      RESULTS   \n",
              "9468   pdt was associated with a significant decrease...  CONCLUSIONS   \n",
              "12269  patients received oral se tablets ( @ mcg ) or...      RESULTS   \n",
              "29097  the primary endpoint was change from baseline ...   BACKGROUND   \n",
              "24512  in contrast , displayed values changed rapidly...      METHODS   \n",
              "12158  in the saffron group , there was no decline in...      METHODS   \n",
              "23202  the prebious trial has the potential to demons...      RESULTS   \n",
              "\n",
              "              pred  pred_prob  pred_correct  \n",
              "13598      RESULTS   0.999860         False  \n",
              "1827    BACKGROUND   0.999752         False  \n",
              "25664      RESULTS   0.999552         False  \n",
              "3573       METHODS   0.999443         False  \n",
              "9468       RESULTS   0.999039         False  \n",
              "12269      METHODS   0.998975         False  \n",
              "29097      METHODS   0.998919         False  \n",
              "24512      RESULTS   0.998911         False  \n",
              "12158      RESULTS   0.998836         False  \n",
              "23202  CONCLUSIONS   0.998668         False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b59ff774-a45b-41dc-8bac-41081de87290\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>pred_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13598</th>\n",
              "      <td>-@ % vs. fish : -@ % vs. fish + s : -@ % ; p &lt;...</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.999860</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1827</th>\n",
              "      <td>nct@ ( clinicaltrials.gov ) .</td>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>0.999752</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25664</th>\n",
              "      <td>rifampicin significantly increased the mean ar...</td>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.999552</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3573</th>\n",
              "      <td>a cluster randomised trial was implemented wit...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.999443</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9468</th>\n",
              "      <td>pdt was associated with a significant decrease...</td>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.999039</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12269</th>\n",
              "      <td>patients received oral se tablets ( @ mcg ) or...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.998975</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29097</th>\n",
              "      <td>the primary endpoint was change from baseline ...</td>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>0.998919</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24512</th>\n",
              "      <td>in contrast , displayed values changed rapidly...</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.998911</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12158</th>\n",
              "      <td>in the saffron group , there was no decline in...</td>\n",
              "      <td>METHODS</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>0.998836</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23202</th>\n",
              "      <td>the prebious trial has the potential to demons...</td>\n",
              "      <td>RESULTS</td>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>0.998668</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b59ff774-a45b-41dc-8bac-41081de87290')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b59ff774-a45b-41dc-8bac-41081de87290 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b59ff774-a45b-41dc-8bac-41081de87290');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a7faac4-766d-4622-b6c6-2f4cddc4005e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a7faac4-766d-4622-b6c6-2f4cddc4005e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a7faac4-766d-4622-b6c6-2f4cddc4005e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "top_100_wrong",
              "summary": "{\n  \"name\": \"top_100_wrong\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"out of @ total measures examined at two different time points , we observed only two significant menstrual phase differences in the subjective response to nicotine .\",\n          \"ethical approval has been received from the national research ethics service ( north east : @/ne/@ for protocol version @ dated @ june @ ) .\",\n          \"to compare the safety and efficacy of dexmedetomidine/propofol ( dp ) - total i.v. anaesthesia ( tiva ) vs remifentanil/propofol ( rp ) - tiva , both with spontaneous breathing , during airway foreign body ( fb ) removal in children .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CONCLUSIONS\",\n          \"OBJECTIVE\",\n          \"RESULTS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"BACKGROUND\",\n          \"OBJECTIVE\",\n          \"METHODS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_prob\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          0.992089033126831,\n          0.9948770403862,\n          0.9931521415710449\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlg-lEkoMb7q"
      },
      "source": [
        "## 4. Check out the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
        "* Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
        "* It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "JOFcg2B7M_2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e87daf-13df-4822-e55c-15d42bc88f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 19:03:20--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 40s  \n",
            "\n",
            "2024-08-14 19:06:02 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading pretrained GloVe embeddings\n",
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "vVVAWUsCGaQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24585abd-65dd-447a-b10d-e96c940699d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# Using the 300-dimensional embeddings to best replicate the paper\n",
        "path_to_glove_file = \"glove.6B.300d.txt\"\n",
        "\n",
        "# Make a dict mapping words to their numpy vector representations\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "f0KPmlZb4OCx"
      },
      "outputs": [],
      "source": [
        "# Create a text vectorization layer\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=68000, # 68k vocab size from paper\n",
        "                                           output_sequence_length=55) # 55-word length covers 95% of examples\n",
        "\n",
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0DauYj0Y8G-7"
      },
      "outputs": [],
      "source": [
        "# Get the total vocabulary of the vectorizer\n",
        "text_vocab = text_vectorizer.get_vocabulary()\n",
        "\n",
        "# Create dict mapping words to their indices\n",
        "word_index = dict(zip(text_vocab, range(len(text_vocab))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "oxRx9iAgJec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23afc321-6c1c-489b-be1a-fd02c81cb4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 29730 words (35111 misses)\n"
          ]
        }
      ],
      "source": [
        "# Preparing an embedding matrix to use with a Keras Embedding layer\n",
        "num_tokens = len(text_vocab) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # Words not found in embedding index will be all-zeros.\n",
        "    # This includes the representation for \"padding\" and \"OOV\"\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    hits += 1\n",
        "  else:\n",
        "    misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "NTr3L5zSy8Py"
      },
      "outputs": [],
      "source": [
        "# Create custom Embedding layer\n",
        "glove_embedding_layer = layers.Embedding(input_dim=num_tokens,\n",
        "                                         output_dim=embedding_dim,\n",
        "                                         embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                         trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "uffA9A8mJTBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c58939-77b7-424e-e2b6-cbf4f6be3a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence before vectorization:\n",
            " australian new zealand clinical trials registry actrn@ .\n",
            "\n",
            "Sentence after vectorization (before embedding):\n",
            "[[1561  319 1691   47  233 1150 1776    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "\n",
            "Sentence after embedding:\n",
            "[[[-0.020175  0.21116  -0.15082  ...  0.39348   0.10561  -0.39483 ]\n",
            "  [-0.62333  -0.42434  -0.035321 ... -0.13613   0.09868   0.609   ]\n",
            "  [-0.21495   0.084593 -0.29487  ...  0.23793   0.48038  -0.33481 ]\n",
            "  ...\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
            "  [ 0.        0.        0.       ...  0.        0.        0.      ]]]\n",
            "\n",
            "Embedded sentence shape: (1, 55, 300)\n"
          ]
        }
      ],
      "source": [
        "# Test out Embedding layer on random sentence\n",
        "import random\n",
        "\n",
        "target_sentence = random.choice(test_sentences)\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "embedded_sentence = glove_embedding_layer(vectorized_sentence)\n",
        "print(f\"Sentence before vectorization:\\n {target_sentence}\\n\")\n",
        "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
        "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
        "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "Ao2SjaqOKALs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51622ee4-5412-4e02-d7bc-986ffd29a587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       multiple                  19452900  \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, None, 128)         192128    \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19826281 (75.63 MB)\n",
            "Trainable params: 373381 (1.42 MB)\n",
            "Non-trainable params: 19452900 (74.21 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create model using custom embedding layer\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "token_embeddings = glove_embedding_layer(inputs)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(token_embeddings)\n",
        "x = layers.MaxPooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling1D(5, padding=\"same\")(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "glove_model = tf.keras.Model(inputs, outputs)\n",
        "glove_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "MRfvXYUorQ4B"
      },
      "outputs": [],
      "source": [
        "train_sen_vectors = text_vectorizer(np.array([[sen] for sen in train_sentences])).numpy()\n",
        "val_sen_vectors = text_vectorizer(np.array([[sen] for sen in val_sentences])).numpy()\n",
        "test_sen_vectors = text_vectorizer(np.array([[sen] for sen in test_sentences])).numpy()\n",
        "\n",
        "# Create datasets for the model\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sen_vectors, train_labels_encoded)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sen_vectors, val_labels_encoded)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sen_vectors, test_labels_encoded)).batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "rQeJ8of0qt1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0ddd89-9681-4d8a-a0fe-3cd3a551f88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5627/5627 [==============================] - 49s 9ms/step - loss: 0.6214 - accuracy: 0.7717 - val_loss: 0.5099 - val_accuracy: 0.8125\n",
            "Epoch 2/100\n",
            "5627/5627 [==============================] - 37s 7ms/step - loss: 0.5011 - accuracy: 0.8187 - val_loss: 0.5075 - val_accuracy: 0.8163\n",
            "Epoch 3/100\n",
            "5627/5627 [==============================] - 45s 8ms/step - loss: 0.4493 - accuracy: 0.8367 - val_loss: 0.5305 - val_accuracy: 0.8168\n",
            "Epoch 4/100\n",
            "5627/5627 [==============================] - 39s 7ms/step - loss: 0.4061 - accuracy: 0.8519 - val_loss: 0.5792 - val_accuracy: 0.8129\n",
            "Epoch 5/100\n",
            "5627/5627 [==============================] - 42s 7ms/step - loss: 0.3736 - accuracy: 0.8641 - val_loss: 0.6309 - val_accuracy: 0.8055\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "glove_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "glove_model_history = glove_model.fit(train_dataset,\n",
        "                                      epochs=100,\n",
        "                                      validation_data=val_dataset,\n",
        "                                      callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "rbDMVOcUrqqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b2cd16-e33e-448e-b61e-e12f73f54cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "942/942 [==============================] - 4s 4ms/step - loss: 0.5260 - accuracy: 0.8094\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5259901285171509, 0.8093578815460205]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "# Evaluate on the test dataset\n",
        "glove_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "s32mph7uwmyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9c3b1c-fca7-42df-907f-85d5f9712130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "942/942 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 80.93578894972623,\n",
              " 'precision': 0.8106525423110673,\n",
              " 'recall': 0.8093578894972623,\n",
              " 'f1': 0.8056769367134935}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "# Make predictions with the model\n",
        "glove_model_pred_probs = glove_model.predict(test_dataset, verbose=1)\n",
        "\n",
        "# Convert pred probs to pred labels\n",
        "glove_model_preds = tf.argmax(glove_model_pred_probs, axis=1)\n",
        "\n",
        "# Calculate results of model_5\n",
        "glove_model_results = calculate_results(y_true=test_labels_encoded,\n",
        "                                        y_pred=glove_model_preds)\n",
        "glove_model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created a simpler model for the GloVe embeddings, and this Conv1D model achieved a respectable ~81% accuracy on the test dataset, compared to ~86.3% for the model in Exercise #1."
      ],
      "metadata": {
        "id": "mB_syHdiV3sa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGkoGVTUMuYq"
      },
      "source": [
        "## 5. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding with the [TensorFlow Hub BERT PubMed expert](https://www.kaggle.com/models/google/experts-bert/tensorFlow2/pubmed/2?tfhub-redirect=true) pretrained embedding (a language model pretrained on PubMed texts). Does this affect the results?\n",
        "* Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://www.kaggle.com/models/google/experts-bert/tensorFlow2/pubmed/2?tfhub-redirect=true)).\n",
        "* Does the BERT model beat the results mentioned in [this paper](https://arxiv.org/pdf/1710.06071.pdf)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ahSmgqFt7BPE"
      },
      "outputs": [],
      "source": [
        "# Import preprocessing and encoder models\n",
        "import tensorflow_text as text\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "preprocessing_layer = hub.KerasLayer('https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3',\n",
        "                                     trainable=False, name=\"pubmed_bert_preprocessor\")\n",
        "\n",
        "bert_layer = hub.KerasLayer('https://www.kaggle.com/models/google/experts-bert/TensorFlow2/pubmed/2',\n",
        "                            trainable=False, name=\"bert_model_layer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "lv8hxEDYQPL_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e18b2e47-5ae4-4104-eae7-b80bf571a49d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Get all keyboard characters\n",
        "import string\n",
        "\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DJXfNKfaQPyg"
      },
      "outputs": [],
      "source": [
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
        "char_vectorizer = layers.TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
        "                                           output_sequence_length=290, # 290 characters covers 95th percentile of sentence lengths\n",
        "                                           name=\"char_vectorizer\")\n",
        "\n",
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CldjDr5iQV_x"
      },
      "outputs": [],
      "source": [
        "# Create character embedding layer\n",
        "char_embed = layers.Embedding(input_dim=len(char_vectorizer.get_vocabulary()), # number of different characters\n",
        "                              output_dim=25, # this is the size of the char embedding in the paper (Figure 1)\n",
        "                              mask_zero=True,\n",
        "                              name=\"char_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hB8TnQB4xLNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4498f6a1-64e3-49a5-c557-0e274281fbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"bert_tribrid_embedding_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " token_inputs (InputLayer)   [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " char_inputs (InputLayer)    [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " pubmed_bert_preprocessor (  {'input_mask': (None, 128)   0         ['token_inputs[0][0]',        \n",
            " KerasLayer)                 , 'input_type_ids': (None,              'char_inputs[0][0]']         \n",
            "                              128),                                                               \n",
            "                              'input_word_ids': (None,                                            \n",
            "                             128)}                                                                \n",
            "                                                                                                  \n",
            " bert_model_layer (KerasLay  {'sequence_output': (None,   1094822   ['pubmed_bert_preprocessor[0][\n",
            " er)                          128, 768),                  41        0]',                          \n",
            "                              'pooled_output': (None, 7              'pubmed_bert_preprocessor[0][\n",
            "                             68),                                   1]',                          \n",
            "                              'default': (None, 768),                'pubmed_bert_preprocessor[0][\n",
            "                              'encoder_outputs': [(None             2]',                          \n",
            "                             , 128, 768),                            'pubmed_bert_preprocessor[1][\n",
            "                              (None, 128, 768),                     0]',                          \n",
            "                              (None, 128, 768),                      'pubmed_bert_preprocessor[1][\n",
            "                              (None, 128, 768),                     1]',                          \n",
            "                              (None, 128, 768),                      'pubmed_bert_preprocessor[1][\n",
            "                              (None, 128, 768),                     2]']                          \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768)]}                                                  \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  98432     ['bert_model_layer[0][13]']   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 128)                  98432     ['bert_model_layer[1][13]']   \n",
            "                                                                                                  \n",
            " char_token_hybrid_embeddin  (None, 256)                  0         ['dense[0][0]',               \n",
            " g (Concatenate)                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " line_number_input (InputLa  [(None, 15)]                 0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " total_lines_input (InputLa  [(None, 20)]                 0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 256)                  65792     ['char_token_hybrid_embedding[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 32)                   512       ['line_number_input[0][0]']   \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 32)                   672       ['total_lines_input[0][0]']   \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " char_token_positional_embe  (None, 320)                  0         ['dense_2[0][0]',             \n",
            " dding (Concatenate)                                                 'dense_3[0][0]',             \n",
            "                                                                     'dropout[0][0]']             \n",
            "                                                                                                  \n",
            " output_layer (Dense)        (None, 5)                    1605      ['char_token_positional_embedd\n",
            "                                                                    ing[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109747686 (418.65 MB)\n",
            "Trainable params: 265445 (1.01 MB)\n",
            "Non-trainable params: 109482241 (417.64 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Recreate model_5 structure with BERT embedding instead of USE\n",
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "bert_inputs = preprocessing_layer(token_inputs)\n",
        "bert_embedding = bert_layer(bert_inputs, training=False)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(bert_embedding[\"pooled_output\"])\n",
        "token_model = tf.keras.Model(inputs=token_inputs, outputs=token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape=[], dtype=\"string\", name=\"char_inputs\")\n",
        "bert_char_inputs = preprocessing_layer(char_inputs)\n",
        "bert_char_embedding = bert_layer(bert_char_inputs, training=False)\n",
        "char_outputs = layers.Dense(128, activation=\"relu\")(bert_char_embedding[\"pooled_output\"])\n",
        "char_model = tf.keras.Model(inputs=char_inputs, outputs=char_outputs)\n",
        "\n",
        "# 3. Line numbers model\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_input\")\n",
        "# dense layer with 32 units & relu activation\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "# combine inputs & dense layer into model\n",
        "line_number_model = tf.keras.Model(inputs=line_number_inputs, outputs=x)\n",
        "\n",
        "# 4. Total lines model\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_lines_input\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_lines_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"char_token_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embeddings with combined token and char embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_model.output,\n",
        "                                                                                total_lines_model.output,\n",
        "                                                                                z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# 8. Put together model with all kinds of inputs\n",
        "bert_model = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                    total_lines_model.input,\n",
        "                                    token_model.input,\n",
        "                                    char_model.input],\n",
        "                            outputs=output_layer,\n",
        "                            name=\"bert_tribrid_embedding_model\")\n",
        "\n",
        "bert_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Pkrftdgm_ZmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3100ca75-951f-4ea7-94d5-ae3ccf1c238e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "35/35 [==============================] - 952s 27s/step - loss: 0.8931 - accuracy: 0.6752 - val_loss: 0.5198 - val_accuracy: 0.8268\n",
            "Epoch 2/3\n",
            "35/35 [==============================] - 926s 27s/step - loss: 0.5107 - accuracy: 0.8243 - val_loss: 0.4454 - val_accuracy: 0.8415\n",
            "Epoch 3/3\n",
            "35/35 [==============================] - 930s 27s/step - loss: 0.4598 - accuracy: 0.8399 - val_loss: 0.4028 - val_accuracy: 0.8493\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "bert_model.compile(loss=\"categorical_crossentropy\",\n",
        "                   optimizer=tf.keras.optimizers.Adam(),\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model on 10% of data for faster training times\n",
        "bert_model_history = bert_model.fit(train_dataset,\n",
        "                                    steps_per_epoch=int(0.1*len(train_dataset)),\n",
        "                                    epochs=3,\n",
        "                                    validation_data=val_dataset,\n",
        "                                    validation_steps=int(0.1*len(val_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Yc84qf2e_3l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c36a44c-81a3-4772-bb6c-8f23f31358f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 1337s 23s/step - loss: 0.4225 - accuracy: 0.8491\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.42246124148368835, 0.8490791320800781]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Evaluate on test dataset\n",
        "bert_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZGkGmvCay-jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78147bcb-4f76-428d-8c2f-616dfe787d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 1328s 23s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 84.90791438526631,\n",
              " 'precision': 0.847216064982828,\n",
              " 'recall': 0.849079143852663,\n",
              " 'f1': 0.847004367308248}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# Make predictions with the model\n",
        "bert_model_pred_probs = bert_model.predict(test_dataset, verbose=1)\n",
        "\n",
        "# Convert pred probs to pred labels\n",
        "bert_model_preds = tf.argmax(bert_model_pred_probs, axis=1)\n",
        "\n",
        "# Calculate results of model_5\n",
        "bert_model_results = calculate_results(y_true=test_labels_encoded,\n",
        "                                       y_pred=bert_model_preds)\n",
        "bert_model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I only trained the BERT model for 3 epochs on 10% of the data because it took SO incredibly long to train. With only a few epochs and limited data this model was able to achieve an F1-score of ~84.7% on the test dataset, which is not as high as the 90% achieved in [this paper](https://arxiv.org/pdf/1710.06071), but it could have improved more if I was able to run the model for longer."
      ],
      "metadata": {
        "id": "tXxgu3LtGRGw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ds-Z_ANbu2"
      },
      "source": [
        "## 6. What happens if you merge the `line_number` and `total_lines` features for each sequence? For example, if you created an `X_of_Y` feature instead? Does this affect model performance?\n",
        "* Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWfY_ZzOC9-"
      },
      "outputs": [],
      "source": [
        "# Combining line numbers and total lines into new DataFrame column\n",
        "train_df[\"line_number_total\"] = train_df[\"line_number\"].astype(str) + \"_of_\" + train_df[\"total_lines\"].astype(str)\n",
        "val_df[\"line_number_total\"] = val_df[\"line_number\"].astype(str) + \"_of_\" + val_df[\"total_lines\"].astype(str)\n",
        "test_df[\"line_number_total\"] = test_df[\"line_number\"].astype(str) + \"_of_\" + test_df[\"total_lines\"].astype(str)\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7Fpzxe4y9RqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48414c05-24ca-4381-ab57-bd5ee7cd55a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((180040, 460), (30212, 460), (30135, 460))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Perform one hot encoding on the train and validation DataFrames\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "# Fitting on the training DataFrame\n",
        "one_hot_encoder.fit(np.expand_dims(train_df[\"line_number_total\"], axis=1))\n",
        "\n",
        "# Transforming both train and val df\n",
        "train_line_number_total_encoded = one_hot_encoder.transform(np.expand_dims(train_df[\"line_number_total\"], axis=1))\n",
        "val_line_number_total_encoded  = one_hot_encoder.transform(np.expand_dims(val_df[\"line_number_total\"], axis=1))\n",
        "test_line_number_total_encoded = one_hot_encoder.transform(np.expand_dims(test_df[\"line_number_total\"], axis=1))\n",
        "\n",
        "# Checking the shapes\n",
        "train_line_number_total_encoded.shape, val_line_number_total_encoded.shape, test_line_number_total_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "0ffF1J5yAffo"
      },
      "outputs": [],
      "source": [
        "# Converting the sparse object to array\n",
        "train_line_number_total_encoded = train_line_number_total_encoded.toarray()\n",
        "val_line_number_total_encoded = val_line_number_total_encoded.toarray()\n",
        "test_line_number_total_encoded = test_line_number_total_encoded.toarray()\n",
        "\n",
        "# Converting the datatype to int\n",
        "train_line_number_total_encoded = tf.cast(train_line_number_total_encoded, dtype=tf.int32)\n",
        "val_line_number_total_encoded = tf.cast(val_line_number_total_encoded, dtype=tf.int32)\n",
        "test_line_number_total_encoded = tf.cast(test_line_number_total_encoded, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JnKJXw9jAgTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f30c1f2-7f2f-46be-ff1c-7e5649300fcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 460), dtype=tf.int32, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 460), dtype=tf.int32, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 460), dtype=tf.int32, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Making the performant datasets for our tribid model\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sentences,\n",
        "                                                 train_chars,\n",
        "                                                 train_line_number_total_encoded))\n",
        "\n",
        "train_labels = tf.data.Dataset.from_tensor_slices(train_labels_encoded)\n",
        "\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val_sentences,\n",
        "                                               val_chars,\n",
        "                                               val_line_number_total_encoded))\n",
        "\n",
        "val_labels = tf.data.Dataset.from_tensor_slices(val_labels_encoded)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sentences,\n",
        "                                                test_chars,\n",
        "                                                test_line_number_total_encoded))\n",
        "\n",
        "test_labels = tf.data.Dataset.from_tensor_slices(test_labels_encoded)\n",
        "\n",
        "# Zipping the data and labels\n",
        "train_dataset = tf.data.Dataset.zip((train_data, train_labels))\n",
        "val_dataset = tf.data.Dataset.zip((val_data, val_labels))\n",
        "test_dataset = tf.data.Dataset.zip((test_data, test_labels))\n",
        "\n",
        "# Applying batch and prefetching\n",
        "train_dataset = train_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "D71kIfanAxcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f05485-d08e-4536-8afa-e709ce900217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " pubmed_bert_preprocessor (  {'input_mask': (None, 128)   0         ['input_4[0][0]',             \n",
            " KerasLayer)                 , 'input_type_ids': (None,              'input_5[0][0]']             \n",
            "                              128),                                                               \n",
            "                              'input_word_ids': (None,                                            \n",
            "                             128)}                                                                \n",
            "                                                                                                  \n",
            " bert_model_layer (KerasLay  {'sequence_output': (None,   1094822   ['pubmed_bert_preprocessor[4][\n",
            " er)                          128, 768),                  41        0]',                          \n",
            "                              'pooled_output': (None, 7              'pubmed_bert_preprocessor[4][\n",
            "                             68),                                   1]',                          \n",
            "                              'default': (None, 768),                'pubmed_bert_preprocessor[4][\n",
            "                              'encoder_outputs': [(None             2]',                          \n",
            "                             , 128, 768),                            'pubmed_bert_preprocessor[5][\n",
            "                              (None, 128, 768),                     0]',                          \n",
            "                              (None, 128, 768),                      'pubmed_bert_preprocessor[5][\n",
            "                              (None, 128, 768),                     1]',                          \n",
            "                              (None, 128, 768),                      'pubmed_bert_preprocessor[5][\n",
            "                              (None, 128, 768),                     2]']                          \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768),                                                   \n",
            "                              (None, 128, 768)]}                                                  \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)        [(None, 460)]                0         []                            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 64)                   49216     ['bert_model_layer[4][13]']   \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 64)                   49216     ['bert_model_layer[5][13]']   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 32)                   14752     ['input_6[0][0]']             \n",
            "                                                                                                  \n",
            " token_char_hybrid_embeddin  (None, 128)                  0         ['dense_10[0][0]',            \n",
            " g (Concatenate)                                                     'dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " tribid_embeddings (Concate  (None, 160)                  0         ['dense_12[0][0]',            \n",
            " nate)                                                               'token_char_hybrid_embedding[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 160)                  0         ['tribid_embeddings[0][0]']   \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 128)                  20608     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 5)                    645       ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109616678 (418.15 MB)\n",
            "Trainable params: 134437 (525.14 KB)\n",
            "Non-trainable params: 109482241 (417.64 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Buidling the tribid model using the functional api\n",
        "input_token = layers.Input(shape=[], dtype=tf.string)\n",
        "bert_inputs_token = preprocessing_layer(input_token)\n",
        "bert_embedding_char =bert_layer(bert_inputs_token)\n",
        "output_token = layers.Dense(64, activation=\"relu\")(bert_embedding_char[\"pooled_output\"])\n",
        "token_model = tf.keras.Model(input_token, output_token)\n",
        "\n",
        "input_char = layers.Input(shape=[], dtype=tf.string)\n",
        "bert_inputs_char = preprocessing_layer(input_char)\n",
        "bert_embedding_char =bert_layer(bert_inputs_char)\n",
        "output_char = layers.Dense(64, activation=\"relu\")(bert_embedding_char[\"pooled_output\"])\n",
        "char_model = tf.keras.Model(input_char, output_char)\n",
        "\n",
        "line_number_total_input = layers.Input(shape=(460,), dtype=tf.int32)\n",
        "dense = layers.Dense(32, activation=\"relu\")(line_number_total_input)\n",
        "total_line_number_model = tf.keras.Model(line_number_total_input, dense)\n",
        "\n",
        "# Concatenating the tokens amd chars output (Hybrid!!!)\n",
        "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "\n",
        "# Combining the line_number_total to our hybrid model (Time for Tribid!!)\n",
        "z = layers.Concatenate(name=\"tribid_embeddings\")([total_line_number_model.output,\n",
        "                                                  combined_embeddings])\n",
        "\n",
        "# Adding a dense + dropout and creating our output layer\n",
        "dropout = layers.Dropout(0.5)(z)\n",
        "x = layers.Dense(128, activation=\"relu\")(dropout)\n",
        "output_layer = layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "# Packing into a model\n",
        "tribrid_model = tf.keras.Model(inputs=[token_model.input,\n",
        "                                       char_model.input,\n",
        "                                       total_line_number_model.input],\n",
        "                               outputs=output_layer)\n",
        "\n",
        "tribrid_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qB2SGMGUBXxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7fb0ff-d9ae-4ab1-8828-b26fcb1b2a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "35/35 [==============================] - 962s 27s/step - loss: 1.0456 - accuracy: 0.6059 - val_loss: 0.6380 - val_accuracy: 0.7822\n",
            "Epoch 2/3\n",
            "35/35 [==============================] - 943s 27s/step - loss: 0.6245 - accuracy: 0.7848 - val_loss: 0.4720 - val_accuracy: 0.8379\n",
            "Epoch 3/3\n",
            "35/35 [==============================] - 934s 27s/step - loss: 0.5310 - accuracy: 0.8141 - val_loss: 0.4273 - val_accuracy: 0.8464\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "tribrid_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "tribrid_model_history = tribrid_model.fit(train_dataset,\n",
        "                                          steps_per_epoch=int(0.1*len(train_dataset)),\n",
        "                                          epochs=3,\n",
        "                                          validation_data=val_dataset,\n",
        "                                          validation_steps=int(0.1*len(val_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "a1V865N2BokW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86b3a8f-751b-4cdc-c4d4-8191ac365d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 1347s 23s/step - loss: 0.4481 - accuracy: 0.8382\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4480772912502289, 0.8381615877151489]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "tribrid_model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the model\n",
        "tribrid_model_pred_probs = tribrid_model.predict(test_dataset, verbose=1)\n",
        "\n",
        "# Convert pred probs to pred labels\n",
        "tribrid_model_preds = tf.argmax(tribrid_model_pred_probs, axis=1)\n",
        "\n",
        "# Calculate results of model_5\n",
        "tribrid_model_results = calculate_results(y_true=test_labels_encoded,\n",
        "                                          y_pred=tribrid_model_preds)\n",
        "tribrid_model_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgTywKeNHNh3",
        "outputId": "b504403a-e4d0-4e61-a316-f710173457e3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 1373s 23s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 83.8161606105857,\n",
              " 'precision': 0.8376790556343388,\n",
              " 'recall': 0.838161606105857,\n",
              " 'f1': 0.8362848053097415}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the `line_number` and `total_lines` features into a single feature seems to have made the model perform slighly worse than keeping them separate, although the difference is only about 1% lower accuracy."
      ],
      "metadata": {
        "id": "hf3ZumyrXsoV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfCMu49SOFE8"
      },
      "source": [
        "## 7. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract, and return the abstract in the format:\n",
        "* `PREDICTED_LABEL`: `SEQUENCE`\n",
        "* `PREDICTED_LABEL`: `SEQUENCE`\n",
        "* `PREDICTED_LABEL`: `SEQUENCE`\n",
        "* `PREDICTED_LABEL`: `SEQUENCE`\n",
        "* ...\n",
        "  * You can find your own unstructured RCT abstract from [PubMed](https://pubmed.ncbi.nlm.nih.gov/) or try [this one](https://pubmed.ncbi.nlm.nih.gov/22244707/), or [these](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "JaSqgRgxZxf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbe79d9-11e1-46de-c01e-2fd853a53fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 18:01:12--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6737 (6.6K) [text/plain]\n",
            "Saving to: ‘skimlit_example_abstracts.json’\n",
            "\n",
            "skimlit_example_abs 100%[===================>]   6.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-08-14 18:01:13 (86.9 MB/s) - ‘skimlit_example_abstracts.json’ saved [6737/6737]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get some example abstracts from GitHub\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "IsAaBqeLCyla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1124889b-4bcb-41b6-b262-a1ab0075bca8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'abstract': 'This RCT examined the efficacy of a manualized social intervention for children with HFASDs. Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition. Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures). Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents. High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity. Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.',\n",
              "  'source': 'https://pubmed.ncbi.nlm.nih.gov/20232240/',\n",
              "  'details': 'RCT of a manualized social treatment for high-functioning autism spectrum disorders'},\n",
              " {'abstract': \"Postpartum depression (PPD) is the most prevalent mood disorder associated with childbirth. No single cause of PPD has been identified, however the increased risk of nutritional deficiencies incurred through the high nutritional requirements of pregnancy may play a role in the pathology of depressive symptoms. Three nutritional interventions have drawn particular interest as possible non-invasive and cost-effective prevention and/or treatment strategies for PPD; omega-3 (n-3) long chain polyunsaturated fatty acids (LCPUFA), vitamin D and overall diet. We searched for meta-analyses of randomised controlled trials (RCT's) of nutritional interventions during the perinatal period with PPD as an outcome, and checked for any trials published subsequently to the meta-analyses. Fish oil: Eleven RCT's of prenatal fish oil supplementation RCT's show null and positive effects on PPD symptoms. Vitamin D: no relevant RCT's were identified, however seven observational studies of maternal vitamin D levels with PPD outcomes showed inconsistent associations. Diet: Two Australian RCT's with dietary advice interventions in pregnancy had a positive and null result on PPD. With the exception of fish oil, few RCT's with nutritional interventions during pregnancy assess PPD. Further research is needed to determine whether nutritional intervention strategies during pregnancy can protect against symptoms of PPD. Given the prevalence of PPD and ease of administering PPD measures, we recommend future prenatal nutritional RCT's include PPD as an outcome.\",\n",
              "  'source': 'https://pubmed.ncbi.nlm.nih.gov/28012571/',\n",
              "  'details': 'Formatting removed (can be used to compare model to actual example)'},\n",
              " {'abstract': 'Mental illness, including depression, anxiety and bipolar disorder, accounts for a significant proportion of global disability and poses a substantial social, economic and heath burden. Treatment is presently dominated by pharmacotherapy, such as antidepressants, and psychotherapy, such as cognitive behavioural therapy; however, such treatments avert less than half of the disease burden, suggesting that additional strategies are needed to prevent and treat mental disorders. There are now consistent mechanistic, observational and interventional data to suggest diet quality may be a modifiable risk factor for mental illness. This review provides an overview of the nutritional psychiatry field. It includes a discussion of the neurobiological mechanisms likely modulated by diet, the use of dietary and nutraceutical interventions in mental disorders, and recommendations for further research. Potential biological pathways related to mental disorders include inflammation, oxidative stress, the gut microbiome, epigenetic modifications and neuroplasticity. Consistent epidemiological evidence, particularly for depression, suggests an association between measures of diet quality and mental health, across multiple populations and age groups; these do not appear to be explained by other demographic, lifestyle factors or reverse causality. Our recently published intervention trial provides preliminary clinical evidence that dietary interventions in clinically diagnosed populations are feasible and can provide significant clinical benefit. Furthermore, nutraceuticals including n-3 fatty acids, folate, S-adenosylmethionine, N-acetyl cysteine and probiotics, among others, are promising avenues for future research. Continued research is now required to investigate the efficacy of intervention studies in large cohorts and within clinically relevant populations, particularly in patients with schizophrenia, bipolar and anxiety disorders.',\n",
              "  'source': 'https://pubmed.ncbi.nlm.nih.gov/28942748/',\n",
              "  'details': 'Effect of nutrition on mental health'},\n",
              " {'abstract': \"Hepatitis C virus (HCV) and alcoholic liver disease (ALD), either alone or in combination, count for more than two thirds of all liver diseases in the Western world. There is no safe level of drinking in HCV-infected patients and the most effective goal for these patients is total abstinence. Baclofen, a GABA(B) receptor agonist, represents a promising pharmacotherapy for alcohol dependence (AD). Previously, we performed a randomized clinical trial (RCT), which demonstrated the safety and efficacy of baclofen in patients affected by AD and cirrhosis. The goal of this post-hoc analysis was to explore baclofen's effect in a subgroup of alcohol-dependent HCV-infected cirrhotic patients. Any patient with HCV infection was selected for this analysis. Among the 84 subjects randomized in the main trial, 24 alcohol-dependent cirrhotic patients had a HCV infection; 12 received baclofen 10mg t.i.d. and 12 received placebo for 12-weeks. With respect to the placebo group (3/12, 25.0%), a significantly higher number of patients who achieved and maintained total alcohol abstinence was found in the baclofen group (10/12, 83.3%; p=0.0123). Furthermore, in the baclofen group, compared to placebo, there was a significantly higher increase in albumin values from baseline (p=0.0132) and a trend toward a significant reduction in INR levels from baseline (p=0.0716). In conclusion, baclofen was safe and significantly more effective than placebo in promoting alcohol abstinence, and improving some Liver Function Tests (LFTs) (i.e. albumin, INR) in alcohol-dependent HCV-infected cirrhotic patients. Baclofen may represent a clinically relevant alcohol pharmacotherapy for these patients.\",\n",
              "  'source': 'https://pubmed.ncbi.nlm.nih.gov/22244707/',\n",
              "  'details': 'Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection'}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# Use json to load in the samples\n",
        "import json\n",
        "with open(\"skimlit_example_abstracts.json\", \"r\") as f:\n",
        "  example_abstracts = json.load(f)\n",
        "\n",
        "example_abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "QLQZvCYdC6fL"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "sample_abstracts = pd.DataFrame(example_abstracts)\n",
        "sample_abstracts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "x_2HH11MDC6C"
      },
      "outputs": [],
      "source": [
        "# Create the function\n",
        "from spacy.lang.en import English\n",
        "\n",
        "def predict_on_abstract(abstract_dict, model, label_encoder):\n",
        "  '''\n",
        "\n",
        "    Takes in a list of dictionaries of abstracts,\n",
        "\n",
        "    [{'abstract': 'This RCT examined .......' ,\n",
        "      'details': 'RCT of a manuali......',\n",
        "      'source': 'https://pubmed.ncbi.nlm........./'},..........]\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "      - abstract_dict : Abstract dictionary of the above format\n",
        "      - model : the trained model on the same data format (line_numbers,  total_lines , sentences , characters)\n",
        "      - label_encoder : the label encoder used to encode the classes\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "      Prints out the predicted label and the corresponding sequence/ text\n",
        "  '''\n",
        "\n",
        "  # Setup english sentence parser\n",
        "  nlp = English()\n",
        "\n",
        "  # Create sentence splitting pipeline object\n",
        "  sentencizer = nlp.add_pipe('sentencizer')\n",
        "\n",
        "  # Create doc of parsed sequences\n",
        "  doc = nlp(abstract_dict['abstract'])\n",
        "\n",
        "  # Return detected sentences from doc in string typpe\n",
        "  abstract_lines = [str(sent) for sent in list(doc.sents)]\n",
        "\n",
        "  # Get total number of lines\n",
        "  total_lines_in_sample = len(abstract_lines)\n",
        "\n",
        "  # Loop through each line in the abstract and create a list of dictionaries containing features\n",
        "  sample_lines = []\n",
        "  for i , line in enumerate(abstract_lines):\n",
        "    sample_dict = {}\n",
        "    sample_dict['text'] = str(line)\n",
        "    sample_dict['line_number'] = i\n",
        "    sample_dict['total_lines'] = total_lines_in_sample - 1\n",
        "    sample_lines.append(sample_dict)\n",
        "\n",
        "\n",
        "  # Get all line number and total lines numbers then one hot encode them\n",
        "  abstract_line_numbers = [line['line_number'] for line in sample_lines]\n",
        "  abstract_total_lines = [line['total_lines'] for line in sample_lines]\n",
        "\n",
        "  abstract_line_numbers_one_hot = tf.one_hot(abstract_line_numbers , depth = 15)\n",
        "  abstract_total_lines_one_hot = tf.one_hot(abstract_total_lines , depth = 20)\n",
        "\n",
        "\n",
        "  # Split the lines into characters\n",
        "  abstract_chars = [\" \".join(list(sentence)) for sentence in abstract_lines]\n",
        "\n",
        "  # Making prediction on sample features\n",
        "  abstract_pred_probs = model.predict(x = (abstract_line_numbers_one_hot,\n",
        "                                           abstract_total_lines_one_hot ,\n",
        "                                           tf.constant(abstract_lines) ,\n",
        "                                           tf.constant(abstract_chars)))\n",
        "\n",
        "  # Turn prediction probs to pred class\n",
        "  abstract_preds = tf.argmax(abstract_pred_probs , axis = 1)\n",
        "\n",
        "  # Prediction class integers into string class name\n",
        "  abstract_pred_classes = [label_encoder.classes_[i] for i in abstract_preds]\n",
        "\n",
        "  # Prints out the abstract lines and the predicted sequence labels\n",
        "  for i , line in enumerate(abstract_lines):\n",
        "    print(f'{abstract_pred_classes[i]}:  {line}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "EVfHlvniDfdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34adec2-d2ba-4bf4-fa07-4bf3679ab88c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 157ms/step\n",
            "OBJECTIVE:  This RCT examined the efficacy of a manualized social intervention for children with HFASDs.\n",
            "\n",
            "METHODS:  Participants were randomly assigned to treatment or wait-list conditions.\n",
            "\n",
            "METHODS:  Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.\n",
            "\n",
            "METHODS:  A response-cost program was applied to reduce problem behaviors and foster skills acquisition.\n",
            "\n",
            "RESULTS:  Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).\n",
            "\n",
            "METHODS:  Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.\n",
            "\n",
            "RESULTS:  High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.\n",
            "\n",
            "RESULTS:  Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.\n",
            "\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "BACKGROUND:  Postpartum depression (PPD) is the most prevalent mood disorder associated with childbirth.\n",
            "\n",
            "BACKGROUND:  No single cause of PPD has been identified, however the increased risk of nutritional deficiencies incurred through the high nutritional requirements of pregnancy may play a role in the pathology of depressive symptoms.\n",
            "\n",
            "METHODS:  Three nutritional interventions have drawn particular interest as possible non-invasive and cost-effective prevention and/or treatment strategies for PPD; omega-3 (n-3) long chain polyunsaturated fatty acids (LCPUFA), vitamin D and overall diet.\n",
            "\n",
            "METHODS:  We searched for meta-analyses of randomised controlled trials (RCT's) of nutritional interventions during the perinatal period with PPD as an outcome, and checked for any trials published subsequently to the meta-analyses.\n",
            "\n",
            "RESULTS:  Fish oil: Eleven RCT's of prenatal fish oil supplementation RCT's show null and positive effects on PPD symptoms.\n",
            "\n",
            "RESULTS:  Vitamin D: no relevant RCT's were identified, however seven observational studies of maternal vitamin D levels with PPD outcomes showed inconsistent associations.\n",
            "\n",
            "RESULTS:  Diet: Two Australian RCT's with dietary advice interventions in pregnancy had a positive and null result on PPD.\n",
            "\n",
            "RESULTS:  With the exception of fish oil, few RCT's with nutritional interventions during pregnancy assess PPD.\n",
            "\n",
            "CONCLUSIONS:  Further research is needed to determine whether nutritional intervention strategies during pregnancy can protect against symptoms of PPD.\n",
            "\n",
            "CONCLUSIONS:  Given the prevalence of PPD and ease of administering PPD measures, we recommend future prenatal nutritional RCT's include PPD as an outcome.\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "BACKGROUND:  Mental illness, including depression, anxiety and bipolar disorder, accounts for a significant proportion of global disability and poses a substantial social, economic and heath burden.\n",
            "\n",
            "BACKGROUND:  Treatment is presently dominated by pharmacotherapy, such as antidepressants, and psychotherapy, such as cognitive behavioural therapy; however, such treatments avert less than half of the disease burden, suggesting that additional strategies are needed to prevent and treat mental disorders.\n",
            "\n",
            "BACKGROUND:  There are now consistent mechanistic, observational and interventional data to suggest diet quality may be a modifiable risk factor for mental illness.\n",
            "\n",
            "BACKGROUND:  This review provides an overview of the nutritional psychiatry field.\n",
            "\n",
            "BACKGROUND:  It includes a discussion of the neurobiological mechanisms likely modulated by diet, the use of dietary and nutraceutical interventions in mental disorders, and recommendations for further research.\n",
            "\n",
            "RESULTS:  Potential biological pathways related to mental disorders include inflammation, oxidative stress, the gut microbiome, epigenetic modifications and neuroplasticity.\n",
            "\n",
            "CONCLUSIONS:  Consistent epidemiological evidence, particularly for depression, suggests an association between measures of diet quality and mental health, across multiple populations and age groups; these do not appear to be explained by other demographic, lifestyle factors or reverse causality.\n",
            "\n",
            "CONCLUSIONS:  Our recently published intervention trial provides preliminary clinical evidence that dietary interventions in clinically diagnosed populations are feasible and can provide significant clinical benefit.\n",
            "\n",
            "CONCLUSIONS:  Furthermore, nutraceuticals including n-3 fatty acids, folate, S-adenosylmethionine, N-acetyl cysteine and probiotics, among others, are promising avenues for future research.\n",
            "\n",
            "CONCLUSIONS:  Continued research is now required to investigate the efficacy of intervention studies in large cohorts and within clinically relevant populations, particularly in patients with schizophrenia, bipolar and anxiety disorders.\n",
            "\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "BACKGROUND:  Hepatitis C virus (HCV) and alcoholic liver disease (ALD), either alone or in combination, count for more than two thirds of all liver diseases in the Western world.\n",
            "\n",
            "BACKGROUND:  There is no safe level of drinking in HCV-infected patients and the most effective goal for these patients is total abstinence.\n",
            "\n",
            "BACKGROUND:  Baclofen, a GABA(B) receptor agonist, represents a promising pharmacotherapy for alcohol dependence (AD).\n",
            "\n",
            "METHODS:  Previously, we performed a randomized clinical trial (RCT), which demonstrated the safety and efficacy of baclofen in patients affected by AD and cirrhosis.\n",
            "\n",
            "BACKGROUND:  The goal of this post-hoc analysis was to explore baclofen's effect in a subgroup of alcohol-dependent HCV-infected cirrhotic patients.\n",
            "\n",
            "RESULTS:  Any patient with HCV infection was selected for this analysis.\n",
            "\n",
            "RESULTS:  Among the 84 subjects randomized in the main trial, 24 alcohol-dependent cirrhotic patients had a HCV infection; 12 received baclofen 10mg t.i.d.\n",
            "\n",
            "METHODS:  and 12 received placebo for 12-weeks.\n",
            "\n",
            "RESULTS:  With respect to the placebo group (3/12, 25.0%), a significantly higher number of patients who achieved and maintained total alcohol abstinence was found in the baclofen group (10/12, 83.3%; p=0.0123).\n",
            "\n",
            "RESULTS:  Furthermore, in the baclofen group, compared to placebo, there was a significantly higher increase in albumin values from baseline (p=0.0132) and a trend toward a significant reduction in INR levels from baseline (p=0.0716).\n",
            "\n",
            "CONCLUSIONS:  In conclusion, baclofen was safe and significantly more effective than placebo in promoting alcohol abstinence, and improving some Liver Function Tests (LFTs) (i.e. albumin, INR) in alcohol-dependent HCV-infected cirrhotic patients.\n",
            "\n",
            "CONCLUSIONS:  Baclofen may represent a clinically relevant alcohol pharmacotherapy for these patients.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for abstract in example_abstracts:\n",
        "  predict_on_abstract(abstract, model_5, label_encoder)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPhxMd5VnN8t2PQa33XWkP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}